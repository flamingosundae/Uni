Efficienza algoritmi basata su (nella maggior parte dei casi) TEMPO e SPAZIO. Entrambi sono calcolabili.
Tempo come finestra entro il quale algoritmo esegue un dato numero di calcoli.


n = input
o = miglior caso
O = peggior caso

E.g. 
Prendimo un contesto dove puoi prendere 100 messaggi alla volta. Quale è o tra 1.000.000, 101, e 99 messaggi?
Ovviamente 99. 101 è rilevante nel senso che RADDOPPIA il tempo di esecuzione.

Su ispezione superficiale, guardiamo la DATA STRUCTURE.
Guardiamo le righe solo se non ci sono istruzioni di branch/jump, ecc.ecc.
La seconda cosa è che guardiamo IL MODO d'uso.
Guardando un x problema, è dato di fatto che alcuni algoritmi siano più efficaci di altri, per esempio per fare un sort.


In una data istruzione x, ogni istruzione richiede O(1).
In un algo che acceda ad un array mille volte, qual è O?